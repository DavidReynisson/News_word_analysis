{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947b99f8",
   "metadata": {},
   "source": [
    "# Step 2: Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa0994",
   "metadata": {},
   "source": [
    "**Project Description**: This project aims to analyze which words in news headlines generate the most engagement. Headlines are from the r/news subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2af09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reyni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\reyni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4') #Download OpenMultilingualWordnet\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12106c26",
   "metadata": {},
   "source": [
    "Before we can proceed with our analysis, we first need to clean our data. Since we are only interested in specific words, that means we need to remove capatalization, special characters, numbers and stop words. Finally, we will use lemmatization to combine words that refer to the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0eb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from step 1\n",
    "titles = pd.read_csv('titles.csv').drop(['Unnamed: 0'], axis=1) # read from csv and drop extra index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9838cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cleaner function to remove stuff we are not interested in\n",
    "def cleaner(document):\n",
    "    document = document.lower() #To lower case\n",
    "    document = re.sub(r'[^\\w\\s]','', document) #Remove non-alphanumeric characters\n",
    "    document =  re.sub(r'[^a-zA-Z ]''+','',document) #remove numbers\n",
    "    return document\n",
    "\n",
    "titles['title'] = titles['title'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fe1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "titles['lemma_titles'] = titles['title'].apply(lambda word: wnl.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220454a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "stop = stopwords.words('english')\n",
    "titles['title_stop'] = titles['lemma_titles'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cc3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop the orignal titles and the lemma_titles and rename \n",
    "news = titles.loc[:, ~titles.columns.isin(['title', 'lemma_titles'])].copy()\n",
    "news.rename(columns = {'title_stop':'titles'}, inplace = True)\n",
    "news_cleaned = news.to_csv('news_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
